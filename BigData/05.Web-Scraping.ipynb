{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "- 필요한 정보를 선택적으로 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import webbrowser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'www.naver.com'\n",
    "webbrowser.open(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naver_search_url = 'https://search.naver.com/search.naver?query='\n",
    "search_word = '파이썬'\n",
    "url = naver_search_url + search_word\n",
    "webbrowser.open_new(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_url = \"www.google.com/#q=\"\n",
    "search_word = 'python'\n",
    "url = google_url + search_word\n",
    "webbrowser.open_new(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <body>\n",
      "  <div>\n",
      "   <span>\n",
      "    <a href=\"http://www.naver.com\">\n",
      "     naver\n",
      "    </a>\n",
      "    <a href=\"https://www.google.com\">\n",
      "     google\n",
      "    </a>\n",
      "    <a href=\"http://www.daum.net/\">\n",
      "     daum\n",
      "    </a>\n",
      "   </span>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "html = \"\"\"<html><body><div><span>\n",
    "            <a href=http://www.naver.com>naver</a>\n",
    "            <a href=https://www.google.com>google</a>\n",
    "            <a href=http://www.daum.net/>daum</a>\n",
    "            </span></div></body></html>\"\"\"\n",
    "\n",
    "# BeautifulSoup를 이용해 HTML 소스를 파싱\n",
    "soup = BeautifulSoup(html, 'lxml') # 'lxml'은 파서\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find\n",
    "- 첫 번째로 나타나는 해당 태그를 찾는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"http://www.naver.com\">naver</a>\n",
      "naver\n"
     ]
    }
   ],
   "source": [
    "# 첫 번째로 나타나는 a 태그를 찾는다.\n",
    "print(soup.find('a'))\n",
    "# 첫 번째로 나타나는 a 태그의 text를 가져온다.\n",
    "print(soup.find('a').get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find_all\n",
    "- 해당 태그를 모두 찾는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"http://www.naver.com\">naver</a>, <a href=\"https://www.google.com\">google</a>, <a href=\"http://www.daum.net/\">daum</a>]\n",
      "naver\n",
      "google\n",
      "daum\n"
     ]
    }
   ],
   "source": [
    "# 모든 a 태그를 찾는다.\n",
    "print(soup.find_all('a'))\n",
    "for site_name in soup.findAll('a'):\n",
    "    print(site_name.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "html2 =\"\"\"\n",
    "<html>\n",
    " <head>\n",
    "  <title>작품과 작가 모음</title>\n",
    " </head> \n",
    " <body>\n",
    "  <h1>책 정보</h1>\n",
    "  <p id = \"book_title\">토지</p>\n",
    "  <p id = \"author\">박경리</p>\n",
    "  <p id = \"book_title\">태백산맥</p>\n",
    "  <p id = \"author\">조정래</p>\n",
    "  <p id = \"book_title\">감옥으로부터의 사색</p>\n",
    "  <p id = \"author\">신영복</p>\n",
    " </body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>작품과 작가 모음</title>\n",
      "<body>\n",
      "<h1>책 정보</h1>\n",
      "<p id=\"book_title\">토지</p>\n",
      "<p id=\"author\">박경리</p>\n",
      "<p id=\"book_title\">태백산맥</p>\n",
      "<p id=\"author\">조정래</p>\n",
      "<p id=\"book_title\">감옥으로부터의 사색</p>\n",
      "<p id=\"author\">신영복</p>\n",
      "</body>\n",
      "<h1>책 정보</h1>\n",
      "========================================\n",
      "<p id=\"book_title\">토지</p>\n",
      "<p id=\"author\">박경리</p>\n",
      "========================================\n",
      "[<p id=\"book_title\">토지</p>, <p id=\"book_title\">태백산맥</p>, <p id=\"book_title\">감옥으로부터의 사색</p>]\n",
      "[<p id=\"author\">박경리</p>, <p id=\"author\">조정래</p>, <p id=\"author\">신영복</p>]\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html2, 'lxml')\n",
    "\n",
    "print(soup.title)\n",
    "print(soup.body)\n",
    "print(soup.body.h1)\n",
    "\n",
    "print('='*40)\n",
    "print(soup.find('p', {\"id\": \"book_title\"}))\n",
    "print(soup.find('p', {\"id\": \"author\"}))\n",
    "\n",
    "print('='*40)\n",
    "print(soup.find_all('p', {\"id\": \"book_title\"}))\n",
    "print(soup.find_all('p', {\"id\": \"author\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토지/박경리\n",
      "태백산맥/조정래\n",
      "감옥으로부터의 사색/신영복\n"
     ]
    }
   ],
   "source": [
    "book_titles = soup.find_all('p', {\"id\": \"book_title\"})\n",
    "authors = soup.find_all('p', {\"id\": \"author\"})\n",
    "for book_title, author in zip(book_titles, authors):\n",
    "    print(book_title.get_text() + '/' + author.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "<!doctype html><html itemscope=\"\" itemtype=\"http://schema.org/WebPage\" lang=\"ko\"><head><meta content\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(\"https://www.google.co.kr\")\n",
    "print(r)\n",
    "print(r.text[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h1>책 정보</h1>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# body안에 있는 h1 태그\n",
    "soup.select('body h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"book_title\">토지</p>,\n",
       " <p id=\"author\">박경리</p>,\n",
       " <p id=\"book_title\">태백산맥</p>,\n",
       " <p id=\"author\">조정래</p>,\n",
       " <p id=\"book_title\">감옥으로부터의 사색</p>,\n",
       " <p id=\"author\">신영복</p>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# body안에 있는 p 태그\n",
    "soup.select('body p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"book_title\">토지</p>,\n",
       " <p id=\"book_title\">태백산맥</p>,\n",
       " <p id=\"book_title\">감옥으로부터의 사색</p>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p태그의 id가 book_title인 것\n",
    "soup.select('p#book_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"author\">박경리</p>, <p id=\"author\">조정래</p>, <p id=\"author\">신영복</p>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p태그의 id가 author 것\n",
    "soup.select('p#author')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a class=\"portal\" href=\"http://www.naver.com\" id=\"naver\">네이버</a>, <a class=\"portal\" href=\"http://www.daum.net\" id=\"daum\">다음</a>]\n",
      "[<a class=\"portal\" href=\"http://www.naver.com\" id=\"naver\">네이버</a>, <a class=\"search\" href=\"http://www.google.com\" id=\"google\">구글</a>, <a class=\"portal\" href=\"http://www.daum.net\" id=\"daum\">다음</a>, <a class=\"government\" href=\"http://www.nl.go.kr\" id=\"nl\">국립중앙도서관</a>]\n",
      "[<a class=\"portal\" href=\"http://www.naver.com\" id=\"naver\">네이버</a>, <a class=\"search\" href=\"http://www.google.com\" id=\"google\">구글</a>, <a class=\"portal\" href=\"http://www.daum.net\" id=\"daum\">다음</a>, <a class=\"government\" href=\"http://www.nl.go.kr\" id=\"nl\">국립중앙도서관</a>]\n",
      "[<a class=\"portal\" href=\"http://www.naver.com\" id=\"naver\">네이버</a>, <a class=\"search\" href=\"http://www.google.com\" id=\"google\">구글</a>, <a class=\"portal\" href=\"http://www.daum.net\" id=\"daum\">다음</a>, <a class=\"government\" href=\"http://www.nl.go.kr\" id=\"nl\">국립중앙도서관</a>]\n"
     ]
    }
   ],
   "source": [
    "f = open(\"D:/JSG/html/example2.html\", encoding='utf-8')\n",
    "\n",
    "html = f.read()\n",
    "f.close()\n",
    "\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "print(soup.select('a.portal'))\n",
    "print(soup.select('body a'))\n",
    "print(soup.select('html a'))\n",
    "print(soup.select('a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## br 태그 처리\n",
    "- soup를 html를 읽을 때 br 태그가 적용이 되지 않기 때문에 함수로 만들어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대한민국헌법\n",
      "제1조 1. 대한민국은 민주공화국이다.2. 대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.\n",
      "제2조 1. 대한민국의 국민이 되는 요건은 법률로 정한다.2. 국가는 법률이 정하는 바에 의하여 재외국민을 보호할 의무를 진다.\n"
     ]
    }
   ],
   "source": [
    "### br 태그가 적용되지 않는 예제\n",
    "\n",
    "f = open(\"D:/JSG/html/example3.html\", encoding='utf-8')\n",
    "\n",
    "html = f.read()\n",
    "f.close()\n",
    "\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "for title in soup.select('p#title'):\n",
    "    print(title.get_text())\n",
    "\n",
    "for content in soup.select('p#content'):\n",
    "    print(content.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# br 태그 적용하기 위해 br태그를 \\n으로 변경br 태그 적용하기 위해 br태그를 \\n으로 변경\n",
    "def replace_newline(soup_html):\n",
    "    br_to_newlines = soup_html.find_all(\"br\")\n",
    "    for br_to_newline in br_to_newlines:\n",
    "        br_to_newline.replace_with(\"\\n\")\n",
    "    return soup_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대한민국헌법 \n",
      "\n",
      "제1조 \n",
      "1. 대한민국은 민주공화국이다.\n",
      "2. 대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다. \n",
      "\n",
      "제2조 \n",
      "1. 대한민국의 국민이 되는 요건은 법률로 정한다.\n",
      "2. 국가는 법률이 정하는 바에 의하여 재외국민을 보호할 의무를 진다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(\"D:/JSG/html/example3.html\", encoding='utf-8')\n",
    "\n",
    "html_source = f.read()\n",
    "f.close()\n",
    "\n",
    "soup = BeautifulSoup(html_source, 'lxml')\n",
    "title = soup.find('p', {'id' : 'title'})\n",
    "contents = soup.find_all('p', {'id': \"content\"})\n",
    "\n",
    "print(title.get_text(), '\\n')\n",
    "\n",
    "for content in contents:\n",
    "    content1 = replace_newline(content)\n",
    "    print(content1.get_text(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Sites in South Korea Crawling In Alexa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Top Sites in South Korea]\n",
      "1: Google.com\n",
      "2: Naver.com\n",
      "3: Youtube.com\n",
      "4: Daum.net\n",
      "5: Tistory.com\n",
      "6: Tmall.com\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.alexa.com/topsites/countries/KR'\n",
    "\n",
    "html_website_ranking = requests.get(url).text\n",
    "soup_website_ranking = BeautifulSoup(html_website_ranking, 'lxml')\n",
    "\n",
    "website_ranking = soup_website_ranking.select('p a')\n",
    "website_ranking_sites = [website_ranking_element.get_text()\n",
    "                        for website_ranking_element in website_ranking[1:]]\n",
    "\n",
    "print(\"[Top Sites in South Korea]\")\n",
    "for k in range(6):\n",
    "    print(\"{0}: {1}\".format(k+1, website_ranking_sites[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Website\n",
      "1   Google.com\n",
      "2    Naver.com\n",
      "3  Youtube.com\n",
      "4     Daum.net\n",
      "5  Tistory.com\n",
      "6    Tmall.com\n"
     ]
    }
   ],
   "source": [
    "website_ranking_dict = {\"Website\": website_ranking_sites}\n",
    "df = pd.DataFrame(website_ranking_dict, columns=['Website'], \n",
    "                  index=range(1, len(website_ranking_sites)+1))\n",
    "print(df[0:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 100 in Naver Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['다시 여기 바닷가',\n",
       " '마리아 (Maria)',\n",
       " 'How You Like That',\n",
       " '여름 안에서 by 싹쓰리 (Feat. 황광희)',\n",
       " 'Summer Hate (Feat. 비)',\n",
       " '보라빛 밤 (pporappippam)',\n",
       " '에잇(Prod.&Feat. SUGA of BTS)',\n",
       " 'Downtown Baby',\n",
       " 'Dolphin',\n",
       " '아로하',\n",
       " '살짝 설렜어 (Nonstop)',\n",
       " '사랑하게 될 줄 알았어',\n",
       " 'Monster',\n",
       " 'MORE & MORE',\n",
       " 'PLAY (Feat. 창모)',\n",
       " '어떻게 이별까지 사랑하겠어, 널 사랑하는 거지',\n",
       " '아무노래',\n",
       " 'Into the I-LAND',\n",
       " 'Memories',\n",
       " '흔들리는 꽃들 속에서 네 샴푸향이 느껴진거야',\n",
       " 'Blueming',\n",
       " '좋은 사람 있으면 소개시켜줘',\n",
       " \"Don't Start Now\",\n",
       " 'METEOR',\n",
       " 'Psycho',\n",
       " 'Dance Monkey',\n",
       " '덤더럼(Dumhdurum)',\n",
       " '화려하지 않은 고백',\n",
       " 'OHIO',\n",
       " '처음처럼',\n",
       " '2002',\n",
       " 'WANNABE',\n",
       " 'Love poem',\n",
       " '나비와 고양이 (Feat. 백현 (BAEKHYUN))',\n",
       " '늦은 밤 너의 집 앞 골목길에서',\n",
       " '시작',\n",
       " '그대 고운 내사랑',\n",
       " 'Paris In The Rain',\n",
       " '환상동화 (Secret Story of the Swan)',\n",
       " '너를 만나',\n",
       " 'Rain On Me',\n",
       " '아직 너의 시간에 살아',\n",
       " '어떻게 지내 (Prod. By VAN.C)',\n",
       " 'Juice',\n",
       " '홀로',\n",
       " 'Apple',\n",
       " '너에게 난, 나에게 넌',\n",
       " 'FIESTA',\n",
       " '밤편지',\n",
       " '오늘따라 비가 와서 그런가 봐']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://music.naver.com/listen/history/index.nhn?type=TOTAL_V2&year=2020&month=07&week=3'\n",
    "\n",
    "html_music = requests.get(url).text\n",
    "soup_music = BeautifulSoup(html_music, 'lxml')\n",
    "\n",
    "titles = soup_music.select('a._title span.ellipsis')\n",
    "music_titles = [title.get_text() for title in titles]\n",
    "music_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 다시 여기 바닷가 / 싹쓰리(유두래곤, 린다G, 비룡)\n",
      "2: 마리아 (Maria) / 화사(Hwa Sa)\n",
      "3: How You Like That / BLACKPINK\n",
      "4: 여름 안에서 by 싹쓰리 (Feat. 황광희) / 싹쓰리(유두래곤, 린다G, 비룡)\n",
      "5: Summer Hate (Feat. 비) / 지코 (ZICO)\n",
      "6: 보라빛 밤 (pporappippam) / 선미\n",
      "7: 에잇(Prod.&Feat. SUGA of BTS) / 아이유(IU)\n"
     ]
    }
   ],
   "source": [
    "artists = soup_music.select('td._artist a')\n",
    "music_artists = [artist.get_text().strip() for artist in artists]\n",
    "\n",
    "for k in range(7):\n",
    "    print(\"{0}: {1} / {2}\".format(k+1, music_titles[k], music_artists[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./music.txt', 'w', encoding='utf-8')\n",
    "for k in range(7):\n",
    "    f.write(\"{0},{1},{2}\\n\".format(k+1, music_titles[k], music_artists[k]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.python.org/static/img/python-logo.png'\n",
    "html_image = requests.get(url)\n",
    "image_file_name = os.path.basename(url)\n",
    "folder = 'D:/JSG/html/download'\n",
    "\n",
    "# 폴더가 없으면 폴더를 만들어 줌\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "    \n",
    "image_path = os.path.join(folder, image_file_name)\n",
    "\n",
    "imageFile = open(image_path, 'wb')\n",
    "# 이미지 데이터를 1000000 바이트씩 나눠서 저장\n",
    "chunk_size = 1000000\n",
    "for chunk in html_image.iter_content(chunk_size):\n",
    "    imageFile.write(chunk)\n",
    "imageFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PICJUMBO Image Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://picjumbo.com/free-stock-photos/nature/'\n",
    "html_picjumbo_image = requests.get(url).text\n",
    "soup_picjumbo_image = BeautifulSoup(html_picjumbo_image, 'lxml')\n",
    "folder = 'D:/JSG/html/download'\n",
    "\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "\n",
    "picjumbo_images = soup_picjumbo_image.select('picture img.image')\n",
    "image_urls = [image_url.get('data-src') for image_url in picjumbo_images]\n",
    "\n",
    "\n",
    "for url in image_urls:\n",
    "    html_image = requests.get(url)\n",
    "    image_file_name = os.path.basename(url)\n",
    "    image_path = os.path.join(folder, image_file_name)  \n",
    "    imageFile = open(image_path,'wb')\n",
    "    chunk_size = 1000000\n",
    "    for chunk in html_image.iter_content(chunk_size):\n",
    "        imageFile.write(chunk)\n",
    "    imageFile.close()\n",
    "\n",
    "# for image_url in soup_picjumbo_image.select('picture img.image'):\n",
    "#     html_image = requests.get(image_url['data-src'])\n",
    "#     image_file_name = os.path.basename(image_url['data-src'])\n",
    "#     image_path = os.path.join(folder, image_file_name)  \n",
    "#     imageFile = open(image_path,'wb')\n",
    "#     chunk_size = 1000000\n",
    "#     for chunk in html_image.iter_content(chunk_size):\n",
    "#         imageFile.write(chunk)\n",
    "#     imageFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url(주소)에서 이미지 주소 추출\n",
    "def get_image_url(url):\n",
    "    html_image_url = requests.get(url).text\n",
    "    soup_image_url = BeautifulSoup(html_image_url, 'lxml')\n",
    "    image_elements = soup_image_url.select('picture img')\n",
    "    if image_elements != None:\n",
    "        image_urls = []\n",
    "        for image_element in image_elements:\n",
    "            image_urls.append(image_element.get('data-src'))\n",
    "        return image_urls\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 폴더를 지정해 이미지 주소에서 이미지 내려받기\n",
    "def download_image(img_folder, img_url):\n",
    "    if(img_url != None):\n",
    "        html_image = requests.get(img_url)\n",
    "        # os.path.basename(URL)는 웹사이트나 폴더가 포함된 파일명에서 파일명만 분리\n",
    "        image_file = open(os.path.join(img_folder, os.path.basename(img_url)), 'wb')\n",
    "        \n",
    "        chunk_size = 100000 # 이미지 데이터를 1000000 바이트씩 나눠서 저장\n",
    "        for chunk in html_image.iter_content(chunk_size):\n",
    "            image_file.write(chunk)\n",
    "        image_file.close()\n",
    "        print(\"이미지 파일명: '{0}'. 내려받기 완료!\".format(os.path.basename(img_url)))\n",
    "    else:\n",
    "        print(\"내려받을 이미지가 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 파일명: 'DSC06401-1080x720.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'D0100448-1080x720.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'DSC00128-1080x720.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'DSC07667-1080x1620.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'DSC09423-1080x720.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'DSC07564-1080x720.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'D0100432-1080x720.jpg'. 내려받기 완료!\n",
      "====================\n",
      "선택한 모든 이미지 내려받기 완료\n"
     ]
    }
   ],
   "source": [
    "url = 'https://picjumbo.com/free-stock-photos/nature/'\n",
    "figure_folder = 'D:/JSG/html/download'\n",
    "\n",
    "picjumbo_image_urls = get_image_url(url) # 이미지 파일의 주소 가져오기\n",
    "num_of_download_image = 7                # 내려받을 이미지 개수 지정\n",
    "# num_of_download_image = len(picjumbo_image_urls) # 전체 이미지를 내려받으려면\n",
    "for k in range(num_of_download_image):\n",
    "    download_image(figure_folder, picjumbo_image_urls[k])\n",
    "print(\"=\"*20)\n",
    "print(\"선택한 모든 이미지 내려받기 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airbnb Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "강남역 조용하고 가격좋은 호텔/₩56,610\n",
      "[J.S House] Great Location in Gangnam! !/₩97,157\n",
      "♡Music House in Gangnam/강남stn 1min/ Happy party♡/₩89,362\n",
      "[도시게하] UrbanExit homestay (룸 #6) 매일방역 식기소독/₩83,947\n",
      "F-1# Gangnam Station Exit 5/Latex,goose bedding/₩69,318\n",
      "W22. Natural Green.Gangnam.2room. 상시소독/₩71,385\n",
      "New Sale  Onjung  in Kangnam Cozy and comfortable/₩56,610\n",
      "< 디자인작업실 >\n",
      "★Gangnam stn 5min ★완벽한 추억 보장./₩160,610\n",
      "[Gangnam] Studio(2rooms, 6beds)/₩124,194\n",
      "K-Grand Hostel Gangnam 1 - Deluxe Twin/₩120,360\n",
      "강남역 도보 1분, 최신 주상복합, 1 min from Gangnam station/₩101,089\n",
      "[당일할인]강남파티룸 [PartyRoom] 이젠 지상에서! 브라이덜샤워전문! 용품구비!/₩110,639\n",
      "Eco Lovely Room for 2 persons 2 (same gender only)/₩103,977\n",
      "[5 STAR Review House/ 1min to Gangnam stn]/₩79,715\n",
      "Welcome to Sam's House(Gangnam stn #5)/₩69,549\n",
      "GreenHouse in GangNam (Subway, large Space, Quiet)/₩141,408\n",
      "GangNam Haven/₩217,336\n",
      "♥️[Female only女士专用]SSuzi's house:) Gangnam, Yeoksam/₩193,453\n",
      "*new open!* gangnam Stn cozy & sweet room two beds/₩52,566\n",
      "Big John's Place - # 203/₩124,822\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.airbnb.co.kr/s/%EA%B0%95%EB%82%A8%EC%97%AD/homes?tab_id=home_tab&refinement_paths%5B%5D=%2Fhomes&checkin=2020-08-05&checkout=2020-08-07&source=structured_search_input_header&search_type=search_query&query=%EA%B0%95%EB%82%A8%EC%97%AD&place_id=ChIJKxs2jFmhfDURPP--kvKavw0&adults=2'\n",
    "\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "rooms_list = soup.select('a._gjfol0')\n",
    "room_prices = soup.select('span._1p7iugi span._krjbj')\n",
    "\n",
    "for room, price in zip(rooms_list, room_prices):\n",
    "    print(\"{0}/{1}\".format(room['aria-label'], price.next_sibling))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
