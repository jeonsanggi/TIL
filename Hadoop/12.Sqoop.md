## Sqoop (SQL to Hadoop)

RDBMS와 HDFS 사이에서 데이터를 주고 받을 수 있게 지원하는 툴이다.

RDBMS에서 HDFS로 데이터를 보내는 것을 Import라 하고, 반대로 HDFS에서 RDBMS로 데이터를 보내는 것을 Export라 한다.

### 1. Sqoop 다운로드

- http://apache.tt.co.kr/sqoop/1.4.7/

```shell
wget http://apache.tt.co.kr/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz
tar zxvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz
rm sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz
```

### 2. Sqoop 설정

```shell
cd sqoop-1.4.7.bin__hadoop-2.6.0/
mv conf/sqoop-env-template.sh conf/sqoop-env.sh
vi sqoop-env.sh
```

```sh
...
...

#Set path to where bin/hadoop is available
export HADOOP_COMMON_HOME=$HADOOP_HOME

#Set path to where hadoop-*-core.jar is available
export HADOOP_MAPRED_HOME=$HADOOP_HOME


...
...
```

### 3. MySQL Connector 설치

https://github.com/jeonsanggi/TIL/blob/master/Hadoop/10.HIVE.md 에서 MySQL Connector 설치 참고

```shell
cp ../apache-hive-2.3.7-bin/lib/mysql-connector-java-8.0.21.jar  lib/
```

### 4. MySQL 테이블 생성

```shell
mysql -u root -p
> create database tpch_db;
> grant all privileges on tpch_db.* to hiveuser@'%';
> flush privileges;
```

### 5. tpc-h 데이터 다운로드

- http://www.tpc.org/tpc_documents_current_versions/download_programs/tools-download-request5.asp?bm_type=TPC-H&bm_vers=2.18.0&mode=CURRENT-ONLY
- 데이터 생성

```shell
cd
mkdir tpc-data
cd tpc-data
mv /mnt/share/download/파일이름 .
unzip 파일이름
ln -s 2.18.0_rc2 tpc-h
cd tpc-data/dbgen
cp makefile.suite makefile
vi makefile
```

```makefile
...
...

CC      = gcc
# Current values for DATABASE are: INFORMIX, DB2, TDAT (Teradata)
#                                  SQLSERVER, SYBASE, ORACLE, VECTORWISE
# Current values for MACHINE are:  ATT, DOS, HP, IBM, ICL, MVS,
#                                  SGI, SUN, U2200, VMS, LINUX, WIN32
# Current values for WORKLOAD are:  TPCH
DATABASE= MYSQL
MACHINE = LINUX
WORKLOAD = TPCH

...
...
```

```shell
make dbgen
# 1기가 정도의 데이터를 생성
./dbgen -s 1
```

dss.ddl를 활용해 테이블을 생성한다.

```shell
mysql -u hiveuser -p tpch_db < dss.ddl
```

테이블 확인

```shell
mysql -u hiveuser -p
> use tpch_db
> show tables;
```

테이블 별 데이터 로드를 위한 설정, local 파일을 입력 받을 수 있게 설정

```mysql
> system mysql -u root -p
> SET GLOBAL Local_infile=1;
> quit
> quit
```

```shell
mysql --local-infile=1 -u hiveuser -p
```

데이터 로드

- customer.tbl
- lineitem.tbl
- nation.tbl
- orders.tbl
- part.tbl
- partsupp.tbl
- region.tbl
- supplier.tbl

```mysql
> use tpch_db;
> load data local infile '/home/bit44/tpc-data/tpc-h/dbgen/customer.tbl' into table CUSTOMER fields terminated b
y '|';
> load data local infile '/home/bit44/tpc-data/tpc-h/dbgen/lineitem.tbl' into table LINEITEM fields terminated b
y '|';
> load data local infile '/home/bit44/tpc-data/tpc-h/dbgen/nation.tbl' into table NATION fields terminated by '|';
> load data local infile '/home/bit44/tpc-data/tpc-h/dbgen/orders.tbl' into table ORDERS fields terminated by '|';
> load data local infile '/home/bit44/tpc-data/tpc-h/dbgen/part.tbl' into table PART fields terminated by '|';
> load data local infile '/home/bit44/tpc-data/tpc-h/dbgen/partsupp.tbl' into table PARTSUPP fields terminated b
y '|';
> load data local infile '/home/bit44/tpc-data/tpc-h/dbgen/region.tbl' into table REGION fields terminated by '|';
> load data local infile '/home/bit44/tpc-data/tpc-h/dbgen/supplier.tbl' into table SUPPLIER fields terminated b
y '|';
```

### 6. Hadoop 실행

```shell
cd
cd hadoop
sbin/start-dfs.sh
sbin/start-yarn.sh
sbin/yarn-daemon.sh start proxyserver
sbin/mr-jobhistory-daemon.sh start historyserver
```

### 7. Sqoop 실행

#### Import

- import 스크립트 생성

```shell
cd
cd sqoop-1.4.7.bin__hadoop-2.6.0/
mkdir scripts
cd scripts
vi supplier_import.sh
```

- DEFAULT로 알파벳 순서대로 컬럼을 정렬함
- --where
  year='2010' 을 이용하여 조건을 줄 수 있음
- --split_by 는 테이블에 키 설정이 안되어 있을 때 키를 강제로 설정 (primary key)
  - split_by는 하나의 키만 가능

```sh
--username
hiveuser
--password
1234
--connect
jdbc:mysql://localhost:3306/tpch_db
--table
SUPPLIER
--columns
S_SUPPKEY,S_NAME,S_ADDRESS,S_NATIONKEY,S_PHONE,S_ACCTBAL,S_COMMENT
--split-by
S_SUPPKEY 
```

- sqoop 실행

```shell
cd ..
bin/sqoop import --options-file scripts/supplier_import.sh
```

- 결과 확인

```shell
cd ~/hadoop
bin/hdfs dfs -ls SUPPLIER
bin/hdfs dfs -tail SUPPLIER/part-m-00000
```

- lineitem

```shell
cd ~/sqoop-1.4.7.bin__hadoop-2.6.0/scripts
vi lineitem_import.sh
```

- -m은 mapreduce 연산 할 task 수
  - --split_by로 키를 지정하면 알아서 병렬 처리를 해줌

```sh
--username
hiveuser
--password
1234
--connect
jdbc:mysql://localhost:3306/tpch_db
--table
LINEITEM
--columns
L_ORDERKEY,L_PARTKEY,L_SUPPKEY,L_LINENUMBER,L_QUANTITY,L_EXTENDEDPRICE,L_DISCOUNT,L_TAX,L_RETURNFLAG,L_LINESTATUS,L_SHIPDATE,L_COMMITDATE,L_RECEIPTDATE,L_SHIPINSTRUCT,L_SHIPMODE,L_COMMENT
-m
1
```

```shell
cd ..
bin/sqoop import --options-file scripts/supplier_import.sh
```

#### Export

```mysql
mysql -u hiveuser -p
> use tpch_db;
> create table carrier_code(code TEXT, description TEXT);
> create table carrier_code_staging like carrier_code;
> quit
```

```shell
cd ~/hadoop
bin/hdfs dfs -mkdir carrier_code
bin/hdfs dfs -put /mnt/share/download/carriers.csv carrier_code
cd ~/sqoop-1.4.7.bin__hadoop-2.6.0/scripts
vi carrier_export.sh
```

- staging-table : 임시 테이블
- clear-staging-table : 임시 테이블의 데이터를 지움
- --input-fields-terminated-by : 구분자 (csv 파일이기때문에 구분자를 ','로 설정)

```sh
--username
hiveuser
--password
1234
--connect
jdbc:mysql://localhost:3306/tpch_db
--table
SUPPLIER
--columns
S_SUPPKEY,S_NAME,S_ADDRESS,S_NATIONKEY,S_PHONE,S_ACCTBAL,S_COMMENT
--input-fields-terminated-by
,
--export-dir
/user/bit44/carrier_code
-m
1
```

```shell
cd ..
bin/sqoop export --options-file scripts/carrier_export.sh
```



### 8. HIVE Import, Export

```sh
--username
hiveuser
-P
--connect
jdbc:mysql://localhost:3306/tpch_db
--table
SUPPLIER
--columns
S_SUPPKEY,S_NAME,S_ADDRESS,S_NATIONKEY,S_PHONE,S_ACCTBAL,S_COMMENT
--split-by
S_SUPPKEY 
--hive-import
--create-hive-table
--hive-table
default.customers
```

```shell
cd ..
bin/sqoop import --options-file scripts/supplier_hive_import.sh
```



#### 8.1 Error 발생시

아래와 같은 에러 발생

> hive.HiveConfig: Could not load org.apache.hadoop.hive.conf.HiveConf. Make sure HIVE_CONF_DIR is set correctly.

1. .bashrc에 HIVE_HOME 추가

```shell
vi ~/.bashrc
```

```.bashrc
...
...

export HIVE_HOME=/home/bit44/apache-hive-2.3.7-bin
```

```shell
source ~/.bashrc
```

2. sqoop-env.sh에 HIVE_HOME 추가

```shell
vi ~/sqoop-1.4.7.bin__hadoop-2.6.0/conf/sqoop-env.sh
```

```shell
...
...
...
#Set the path to where bin/hive is available
export HIVE_HOME=$HIVE_HOME
```

3. hive-common-*.jar 파일 복사

```shell
cp ~/apache-hive-2.3.7-bin/lib/hive-common-*.jar ~/sqoop-1.4.7.bin__hadoop-2.6.0/lib
```

4. 그래도 안되면 reboot하고 하둡 다시 실행

#### 8.2 결과 확인

```
cd ~/apache-hive-2.3.7-bin
bin/hive
> select * from customers limit 10;
```