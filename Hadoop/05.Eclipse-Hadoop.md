# Eclipse Hadoop

## 1.WordCount 예제

### 1.1 Maven Project 생성

- File - New - Other... - Maven - Maven Project

### 1.2 build & dependencies 추가

[Maven Repository](https://mvnrepository.com/)에서 의존성 정보를 알 수 있다. maven은 build하면 target 폴더에 jar 파일을 만들어 줌

- hadoop-common
- hadoop-mapreduce-client-core : client에서 mapreduce 작업을 할 수 있게 해줌

```xml
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
  <modelVersion>4.0.0</modelVersion>
  <groupId>com.jremind</groupId>
  <artifactId>com.jremind</artifactId>
  <version>0.0.1</version>
  
  <build>
  	<plugins>
  		<plugin>
  			<groupId>org.apache.maven.plugins</groupId>
  			<artifactId>maven-compiler-plugin</artifactId>
  			<version>3.6.1</version>
  			<configuration>
  				<source>1.8</source>
  				<target>1.8</target>
  			</configuration>
  		</plugin>
  	</plugins>
  </build>
  
  <dependencies>
  	<!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common -->
	<dependency>
	    <groupId>org.apache.hadoop</groupId>
	    <artifactId>hadoop-common</artifactId>
	    <version>2.7.2</version>
	</dependency>
	<!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-core -->
	<dependency>
	    <groupId>org.apache.hadoop</groupId>
	    <artifactId>hadoop-mapreduce-client-core</artifactId>
	    <version>2.7.2</version>
	</dependency>
  </dependencies>
</project>
```



### 1.3 WordCount MapReduce 구현

Github 코드 : https://github.com/jeonsanggi/TIL/tree/master/Hadoop/com.jremind

#### 1.3.1 디렉토리 구조

![image-20200820135727928](https://github.com/jeonsanggi/TIL/blob/master/Image/Hadoop/디렉토리구조.png)

#### 1.3.2 WordCountMapper.java

```java
package com.jremind.map;

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class WordCountMapper extends 
	Mapper<LongWritable, Text, Text, IntWritable>{
	
	// IntWritable는 하둡에서 제공하는 데이터타입이다. IntWritable으로  Serialize를 제공한다.
	private final static IntWritable one = new IntWritable(1);
	private Text word = new Text();
	
	@Override
	protected void map(LongWritable key, Text value, 
			Mapper<LongWritable, Text, Text, IntWritable>.Context context)
			throws IOException, InterruptedException {
		
		StringTokenizer strToken = new StringTokenizer(value.toString());
		while (strToken.hasMoreTokens()) {
			word.set(strToken.nextToken());
			context.write(word, one);
		}
	}
}
```

#### 1.3.3 WordCountReducer.java

```java
package com.jremind.reduce;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class WordCountReducer extends 
	Reducer<Text, IntWritable, Text, IntWritable>{
	
	private IntWritable result = new IntWritable();

	@Override
	protected void reduce(Text key, Iterable<IntWritable> values,
			Reducer<Text, IntWritable, Text, IntWritable>.Context context) throws IOException, InterruptedException {
		// TODO Auto-generated method stub
		int sum = 0;
		for(IntWritable value : values) {
			sum += value.get();
		}
		
		result.set(sum);
		context.write(key, result);
	}
}
```

#### 1.3.4 WordCount.java

```java
package com.jremind.driver;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

import com.jremind.map.WordCountMapper;
import com.jremind.reduce.WordCountReducer;


public class WordCount {
	public static void main(String[] args) throws Exception{
		// Configuration Job을 생성할때 Job이 필요한 요소들을 저장할 때 필요한 것들을 저쟁해놓음
		Configuration conf = new Configuration();
		
		if (args.length != 2) {
			System.out.println("Usage: WordCount <input> <output>");
			System.exit(2);
		}
		
		// 싱글턴 패턴
		Job job = Job.getInstance(conf, "WordCount");
		
		job.setJarByClass(WordCount.class);				// main으로 들어가는 클래스
		job.setMapperClass(WordCountMapper.class); 	// mapper로 들어가는 클래스
		job.setReducerClass(WordCountReducer.class); 	// Reduce로 들어가는 클래스
		
		job.setInputFormatClass(TextInputFormat.class);	 // 텍스트 인풋
		job.setOutputFormatClass(TextOutputFormat.class);// 텍스트 인풋
		
		job.setOutputKeyClass(Text.class);			// output key 클래스는 Text
		job.setOutputValueClass(IntWritable.class); // output value 클래스는 숫자

		FileInputFormat.addInputPath(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		
		job.waitForCompletion(true);
	}
}
```

#### 1.3.5 Maven build

- Run As - maven build... - Goals: clean install - Run

- build시 아래와 같은 에러 발생 시 pom.xml에 property 추가
  - Source option 5 is no longer supported. Use 6 or later
  - Target option 1.5 is no longer supported. Use 1.6 or later

```xml
...
...
...
  <dependencies>
  ...
  ...
  </dependencies>
  <properties>
      <maven.compiler.source>1.8</maven.compiler.source>
      <maven.compiler.target>1.8</maven.compiler.target>
  </properties>
</project>
```

## 2. 항공 데이터 WordCount

데이터 : http://stat-computing.org/dataexpo/2009/the-data.html
데이터 : https://packages.revolutionanalytics.com/datasets/AirOnTime87to12/
데이터 컬럼 설명 https://docs.microsoft.com/en-us/machine-learning-server/r-reference/revoscaler/airontime87to12

Github 코드 : https://github.com/jeonsanggi/TIL/tree/master/Hadoop/airline

### 2.1 build & dependencies 추가

```xml
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
  <modelVersion>4.0.0</modelVersion>
  <groupId>com.jremind</groupId>
  <artifactId>airline</artifactId>
  <version>0.0.1</version>
  
  <build>
  	<plugins>
  		<plugin>
  			<groupId>org.apache.maven.plugins</groupId>
  			<artifactId>maven-compiler-plugin</artifactId>
  			<version>3.6.1</version>
  			<configuration>
  				<source>1.8</source>
  				<target>1.8</target>
  			</configuration>
  		</plugin>
  	</plugins>
  </build>
  
  <dependencies>
  	<!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common -->
	<dependency>
	    <groupId>org.apache.hadoop</groupId>
	    <artifactId>hadoop-common</artifactId>
	    <version>2.7.2</version>
	</dependency>
	<!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-core -->
	<dependency>
	    <groupId>org.apache.hadoop</groupId>
	    <artifactId>hadoop-mapreduce-client-core</artifactId>
	    <version>2.7.2</version>
	</dependency>
  </dependencies>
</project>
```

### 2.2 MapReduce 구현

#### 2.2.1 디렉토리 구조

![image-20200821140850441](https://github.com/jeonsanggi/TIL/blob/master/Image/Hadoop/image-20200821140850441.png)

#### 2.2.2 AirlinePerformanceParser.java

```java
package com.jremind.hadoop.common;

import org.apache.hadoop.io.Text;

public class AirlinePerformanceParser {
	private int year;
	private int month;
	private int day;
	
	private int arriveDelayTime = 0;
	private int departureDelayTime = 0;
	private int distance = 0;
	
	private boolean arriveDelayAvailable = true;
	private boolean departureDelayAvailable = true;
	private boolean distanceAvailable = true;
	
	private String uniqueCarrier;

	public AirlinePerformanceParser(Text text) {
		try {
			String[] columns = text.toString().split(",");
			
			year = Integer.parseInt(columns[0]);
			month = Integer.parseInt(columns[1]);
			day = Integer.parseInt(columns[2]);
			uniqueCarrier = columns[5];
			
			if(!columns[17].equals("")) {
				departureDelayTime = (int)Float.parseFloat(columns[17]);
			}else {
				departureDelayAvailable = false;
			}
			
			if(!columns[27].equals("")) {
				arriveDelayTime = (int)Float.parseFloat(columns[27]);
			}else {
				arriveDelayAvailable = false;
			}
			
			if(!columns[37].equals("")) {
				distance = (int)Float.parseFloat(columns[37]);
			}else {
				distanceAvailable = false;
			}
			
		}catch (Exception e) {
			// TODO: handle exception
			System.out.println("Error parsing a record:" + e.getMessage());
		}
		
	}

	public int getYear() {
		return year;
	}

	public int getMonth() {
		return month;
	}

	public int getArriveDelayTime() {
		return arriveDelayTime;
	}

	public int getDepartureDelayTime() {
		return departureDelayTime;
	}

	public int getDistance() {
		return distance;
	}

	public boolean isArriveDelayAvailable() {
		return arriveDelayAvailable;
	}

	public boolean isDepartureDelayAvailable() {
		return departureDelayAvailable;
	}

	public boolean isDistanceAvailable() {
		return distanceAvailable;
	}

	public String getUniqueCarrier() {
		return uniqueCarrier;
	}	
}
```

#### 2.2.3 DepartureDelayCountMapper.java

```java
package com.jremind.hadoop.mapper;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import com.jremind.hadoop.common.AirlinePerformanceParser;

public class DepartureDelayCountMapper extends 
	Mapper<LongWritable, Text, Text, IntWritable>{
	
	private final static IntWritable outputValue = new IntWritable(1);
	private Text outputKey = new Text();
	
	@Override
	protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, IntWritable>.Context context)
			throws IOException, InterruptedException {
		// TODO Auto-generated method stub
		AirlinePerformanceParser parser = new AirlinePerformanceParser(value);
		
		outputKey.set(parser.getYear() + "," + parser.getMonth());
		
		if(parser.getDepartureDelayTime() > 0) {
			context.write(outputKey, outputValue);
		}
	}
}
```

#### 2.2.4 DelayCountReducer.java

```java
package com.jremind.hadoop.reducer;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class DelayCountReducer extends 
	Reducer<Text, IntWritable, Text, IntWritable>{
	
	private IntWritable result = new IntWritable();

	@Override
	protected void reduce(Text key, Iterable<IntWritable> values,
			Reducer<Text, IntWritable, Text, IntWritable>.Context context) throws IOException, InterruptedException {
		// TODO Auto-generated method stub
		int sum = 0;
		
		for(IntWritable value : values) {
			sum += value.get();
		}
		
		result.set(sum);
		context.write(key, result);
	}
}
```

#### DepartureDelayCount.java

```java
package com.jremind.hadoop.driver;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

import com.jremind.hadoop.mapper.DepartureDelayCountMapper;
import com.jremind.hadoop.reducer.DelayCountReducer;


public class DepartureDelayCount {
	public static void main(String[] args) throws Exception{
		Configuration conf = new Configuration();
		if (args.length != 2) {
			System.out.println("Usage: DepartureDelayCount <input> <output>");
			System.exit(2);
		}
		
		Job job = Job.getInstance(conf, "DepartureDelayCount");
		
		FileInputFormat.addInputPath(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		
		job.setJarByClass(DepartureDelayCount.class);
		job.setMapperClass(DepartureDelayCountMapper.class);
		job.setReducerClass(DelayCountReducer.class);
		
		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);
		
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		
		job.waitForCompletion(true);
	}
}
```

